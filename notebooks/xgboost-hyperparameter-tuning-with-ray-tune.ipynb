{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# XGBoost Hyperparameter-Tuning with Ray Tune\n",
    "\n",
    "This notebook demonstrates how a XGBoost Classifier (using the Sklearn-API) can be tuned with **Ray Tune**.\n",
    "\n",
    "We will use the Optuna Search Algorithm and the ASHA Scheduler for aggressive early stopping of bad trials.\n",
    "\n",
    "* [Data Loading and Preprocessing](#loading-preprocessing)\n",
    "* [Model Training and Hyperparameter-Optimization](#training-optim)\n",
    "    - [Step 1: Define the parameter space](#parameter-space)\n",
    "    - [Step 2: Define the objective function](#objective)\n",
    "    - [Step 3: Define Search Algorithm and Scheduler](#search-scheduler)\n",
    "    - [Step 4: Define the Tuner object and run the optimization](#tune)\n",
    "    - [Step 5: Evaluate the results](#evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.integration.xgboost import TuneReportCallback\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"loading-preprocessing\"></a>\n",
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/playground-series-s3e5/train.csv\", index_col=0)\n",
    "test = pd.read_csv(\"/kaggle/input/playground-series-s3e5/test.csv\", index_col=0)\n",
    "sample_submission = pd.read_csv(\n",
    "    \"/kaggle/input/playground-series-s3e5/sample_submission.csv\", index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original dataset\n",
    "original = pd.read_csv(\"/kaggle/input/wine-quality-dataset/WineQT.csv\", index_col=\"Id\")\n",
    "original_red = pd.read_csv(\n",
    "    \"/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\"\n",
    ")\n",
    "train = pd.concat([train, original, original_red], axis=0).reset_index(drop=True)\n",
    "train = train[~train.duplicated()]\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"quality\"\n",
    "\n",
    "X = train.drop(columns=target)\n",
    "y = train.loc[:, target]\n",
    "y -= 3\n",
    "\n",
    "X_test = test.copy()\n",
    "\n",
    "# apply standard scaling\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y = y.astype(\"long\")\n",
    "X = X.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "print(\"Train samples: \", X.shape[0])\n",
    "print(\"Test samples: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"training-optim\"></a>\n",
    "# Model Training and Hyperparameter Optimization\n",
    "\n",
    "We will now train a `XGBClassifier` and tune its hyperparameters with ray tune.\n",
    "\n",
    "\n",
    "1. Define the Parameter Space\n",
    "2. Define the objective\n",
    "2. Define Search Algorithm and Scheduler\n",
    "4. Define a Tuner Object\n",
    "5. Evaluate Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"parameter-space\"></a>\n",
    "## Step 1: Define Parameter Space\n",
    "\n",
    "We define the parameter space using the functions provided by tune:\n",
    "\n",
    "For example:\n",
    "- A random integer in a given interval (discrete uniform distribution) can be specified with `tune.randint(low, high)`\n",
    "- A random float in a given interval (continuous uniform distribution) can be specified with `tune.uniform(low, high)`\n",
    "\n",
    "For more information, see here: https://docs.ray.io/en/latest/tune/api/search_space.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"early_stopping_rounds\": 20,\n",
    "    \"eval_metric\": \"mlogloss\",  # mlogloss is the multi-class negative log-likelihood\n",
    "    \"n_estimators\": tune.randint(200, 600),\n",
    "    \"gamma\": tune.randint(1, 5),\n",
    "    \"max_depth\": tune.randint(2, 9),\n",
    "    \"min_child_weight\": tune.randint(1, 5),\n",
    "    \"subsample\": tune.uniform(0.5, 1.0),\n",
    "    \"eta\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"colsample_bytree\": tune.uniform(0.5, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"objective\"></a>\n",
    "## Step 2: Define the objective function\n",
    "\n",
    "This function trains a single classifier and takes in a `config`.\n",
    "It also reports the metrics to the Tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(config):\n",
    "    \"\"\"Objective to be optimized.\n",
    "\n",
    "    Uses a simple 0.8/0.2 train-validation-split and logs the validation logloss using the `TuneReportCallback`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config: dict\n",
    "        The config object.\n",
    "    \"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, stratify=y, shuffle=True, test_size=0.2\n",
    "    )\n",
    "    trc = TuneReportCallback({\"loss\": \"validation_0-mlogloss\"})\n",
    "    clf = XGBClassifier(**config, callbacks=[trc]).fit(\n",
    "        X_train, y_train, eval_set=[(X_val, y_val)], verbose=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"search-scheduler\"></a>\n",
    "## Step 3: Search Algorithm and Scheduler\n",
    "\n",
    "We will use Optuna's Search Algorithm, combined with the ASHA Scheduler.\n",
    "\n",
    "Note that with Ray Tune, it is really easy to switch out both of them. Ray Tune supports many more search algorithms (see https://docs.ray.io/en/latest/tune/api/suggestion.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ASHAScheduler(grace_period=10, reduction_factor=3)\n",
    "\n",
    "search_alg = OptunaSearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tune\"></a>\n",
    "## Step 4: Define the Tuner object and run the optimization\n",
    "\n",
    "We specify the objective, the parameter space and addionitional parameters via the `tune.TuneConfig`.\n",
    "\n",
    "We can specify how many samples the tuning should use. Here we also specify the scheduler and the search algorithm.\n",
    "\n",
    "In this example, we use `num_samples=500`, e.g. 500 trials will be executed. This takes approx. 300s (or 5min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tune.Tuner(\n",
    "    objective,\n",
    "    param_space=param_space,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        num_samples=500,\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        scheduler=scheduler,\n",
    "        search_alg=search_alg,\n",
    "    ),\n",
    ")\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluate\"></a>\n",
    "## Step 5: Evaluate the results of the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "for result in results:\n",
    "    result.metrics_dataframe.plot(\"training_iteration\", \"loss\", ax=ax, legend=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how a lot of bad performing trials get stopped early on by the ASHA Scheduler. This allows for efficient search on a lot of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = results.get_best_result(\"loss\", mode=\"min\").config\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally: Train the classifier on the full data set with best parameter config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier(**best_params).fit(X, y, eval_set=[(X, y)], verbose=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = sample_submission.copy()\n",
    "sub[target] = clf.predict(X_test)\n",
    "sub += 3\n",
    "sub.to_csv(\"submission.csv\")\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
